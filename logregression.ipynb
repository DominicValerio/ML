{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5f6f9e",
   "metadata": {},
   "source": [
    "# Supervised Learning: Logistic Regression\n",
    "\n",
    "Prediction Function\n",
    "$$\\hat{y} = \\sigma(z)$$\n",
    "\n",
    "Activation Function - Sigmoid\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "- Turns the logit's linear score into a probability \n",
    "\n",
    "Score function - Logit\n",
    "$$z = \\mathbf{w} \\cdot \\mathbf{x} + b$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\n",
    "  &\\mathbf{w}\\text{: weight vector}\\\\\n",
    "  &\\mathbf{x}\\text{: input feature vector}\\\\\n",
    "  &b\\text{: bias term}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "A linear combination of the weights and features + bias\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model parameters\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 1000\n",
    "\n",
    "def prediction_function(z):\n",
    "    return (1) / (1+np.exp(-z))\n",
    "\n",
    "def score_function(w, x, b):\n",
    "    return np.dot(x, w) + b\n",
    "\n",
    "weights = []\n",
    "bias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9491fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the training data\n",
    "\n",
    "# [kills, deaths]\n",
    "features = [[3, 6], [7,7], [2,4], [3,5], [0,5], [4, 5], [2, 4], [0, 4]] \n",
    "# [win/loss]\n",
    "labels = [1, 1, 1, 0, 1, 1, 1, 0]\n",
    "\n",
    "# convert to np arrays for matrix and vector operations\n",
    "features = np.array(features); labels = np.array(labels); weights = np.array(np.zeros(features.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18d34f",
   "metadata": {},
   "source": [
    "Loss Function - Cross Entropy\n",
    "$$\\mathcal{L}(y, \\hat{y}) = -\\left[ y \\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y}) \\right]$$\n",
    "\n",
    "- Loss is a number indicating how\n",
    "bad the model's prediction was on a\n",
    "single example. If the model's\n",
    "prediction is perfect, the loss is zero;\n",
    "otherwise, the loss is greater.\n",
    "\n",
    "- The goal is to have weights and biases that have low\n",
    "loss, on average, across all samples. In other words, trying to minimize the cost function.\n",
    "\n",
    "Cost Function\n",
    "\n",
    "$$J(\\mathbf{w}, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)})\\log(1 - \\hat{y}^{(i)}) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc77075",
   "metadata": {},
   "source": [
    "In Gradient Descent: \n",
    " \n",
    "$$\\mathcal{L}(y, \\hat{y})= \\mathcal{L}(y, f(\\mathbf{w}, \\mathbf{x}, b))$$\n",
    "\n",
    "$ y $ and $\\mathbf{x}$ are both constant to this function so you just need\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "&\\nabla L = \\left( \\frac{\\partial L}{\\partial \\mathbf{w}}, \\frac{\\partial L}{\\partial b} \\right)\\\\\n",
    "&\\frac{\\partial L}{\\partial \\mathbf{w}} = (\\hat{y} - y) \\mathbf{x}, \\space \\frac{\\partial L}{\\partial b} = \\hat{y} - y\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "344c269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using gradient descent \n",
    "n = len(features)\n",
    "\n",
    "for _ in range(EPOCHS):\n",
    "    # compute the prediction function\n",
    "    z = score_function(weights, features, bias)\n",
    "    y_hat = prediction_function(z)\n",
    "\n",
    "    # compute gradient components\n",
    "    dL_dw = np.dot(features.T, (y_hat - labels) )\n",
    "    dL_db = y_hat - labels\n",
    "    \n",
    "    dLavg_dw = 1/n * dL_dw\n",
    "    dLavg_db = 1/n * np.sum(dL_db)\n",
    "\n",
    "    # update weights and bias\n",
    "    weights += -dLavg_dw * LEARNING_RATE\n",
    "    bias += -dLavg_db * LEARNING_RATE\n",
    "    # since differentials are the direction where loss increases the most,\n",
    "    # move in the opposite direction of those differentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b4da86f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: 0.36 0.19 \n",
      "Bias: -0.58\n",
      "\n",
      "X testing data: [[3, 6], [0, 2], [3, 5]]\n",
      "Probability of elements: 0.84 0.45 0.81 \n",
      "Prediction of successes: [1 0 1]\n"
     ]
    }
   ],
   "source": [
    "def predict_probability(X_test):\n",
    "    return prediction_function(score_function(weights, X_test, bias))\n",
    "\n",
    "def predict_successes(X_test):\n",
    "    return np.where(predict_probability(X_test) >= 0.5, 1, 0)\n",
    "\n",
    "# note: The magnitude of the weights can tell you how much a feature affects the prediction\n",
    "print(f\"Weights: \", end=\"\")\n",
    "for v in weights:\n",
    "    print(f\"{v:.2f} \", end=\"\")\n",
    "print()\n",
    "print(f\"Bias: {bias:.2f}\\n\")\n",
    "\n",
    "X_test = [ [3,6], [0, 2], [3,5] ]\n",
    "print(\"X testing data: \", end=\"\"); print(X_test)\n",
    "\n",
    "print(f\"Probability of elements: \", end=\"\")\n",
    "for v in X_test:\n",
    "    print(f\"{predict_probability(v):.2f} \", end=\"\")\n",
    "print()\n",
    "\n",
    "print(f\"Prediction of successes: {predict_successes(X_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
